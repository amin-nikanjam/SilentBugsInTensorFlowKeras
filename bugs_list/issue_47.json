{
  "url": "https://api.github.com/repos/tensorflow/tensorflow/issues/40638",
  "repository_url": "https://api.github.com/repos/tensorflow/tensorflow",
  "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/40638/labels{/name}",
  "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/40638/comments",
  "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/40638/events",
  "html_url": "https://github.com/tensorflow/tensorflow/issues/40638",
  "id": 642451845,
  "node_id": "MDU6SXNzdWU2NDI0NTE4NDU=",
  "number": 40638,
  "title": "Keras layer weights/sublayers getting deleted when creating a model with them. model.summary() / plot_model still shows those weights as part of graph though",
  "user": {
    "login": "Santosh-Gupta",
    "id": 5524261,
    "node_id": "MDQ6VXNlcjU1MjQyNjE=",
    "avatar_url": "https://avatars.githubusercontent.com/u/5524261?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/Santosh-Gupta",
    "html_url": "https://github.com/Santosh-Gupta",
    "followers_url": "https://api.github.com/users/Santosh-Gupta/followers",
    "following_url": "https://api.github.com/users/Santosh-Gupta/following{/other_user}",
    "gists_url": "https://api.github.com/users/Santosh-Gupta/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/Santosh-Gupta/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/Santosh-Gupta/subscriptions",
    "organizations_url": "https://api.github.com/users/Santosh-Gupta/orgs",
    "repos_url": "https://api.github.com/users/Santosh-Gupta/repos",
    "events_url": "https://api.github.com/users/Santosh-Gupta/events{/privacy}",
    "received_events_url": "https://api.github.com/users/Santosh-Gupta/received_events",
    "type": "User",
    "site_admin": false
  },
  "labels": [
    {
      "id": 473172988,
      "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=",
      "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug",
      "name": "type:bug",
      "color": "159b2e",
      "default": false,
      "description": "Bug"
    },
    {
      "id": 1097546578,
      "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4",
      "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras",
      "name": "comp:keras",
      "color": "0052cc",
      "default": false,
      "description": "Keras related issues"
    },
    {
      "id": 1903591931,
      "node_id": "MDU6TGFiZWwxOTAzNTkxOTMx",
      "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/TF%202.2",
      "name": "TF 2.2",
      "color": "0052cc",
      "default": false,
      "description": "Issues related to TF 2.2"
    }
  ],
  "state": "closed",
  "locked": false,
  "assignee": {
    "login": "tomerk",
    "id": 676357,
    "node_id": "MDQ6VXNlcjY3NjM1Nw==",
    "avatar_url": "https://avatars.githubusercontent.com/u/676357?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/tomerk",
    "html_url": "https://github.com/tomerk",
    "followers_url": "https://api.github.com/users/tomerk/followers",
    "following_url": "https://api.github.com/users/tomerk/following{/other_user}",
    "gists_url": "https://api.github.com/users/tomerk/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/tomerk/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/tomerk/subscriptions",
    "organizations_url": "https://api.github.com/users/tomerk/orgs",
    "repos_url": "https://api.github.com/users/tomerk/repos",
    "events_url": "https://api.github.com/users/tomerk/events{/privacy}",
    "received_events_url": "https://api.github.com/users/tomerk/received_events",
    "type": "User",
    "site_admin": false
  },
  "assignees": [
    {
      "login": "tomerk",
      "id": 676357,
      "node_id": "MDQ6VXNlcjY3NjM1Nw==",
      "avatar_url": "https://avatars.githubusercontent.com/u/676357?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/tomerk",
      "html_url": "https://github.com/tomerk",
      "followers_url": "https://api.github.com/users/tomerk/followers",
      "following_url": "https://api.github.com/users/tomerk/following{/other_user}",
      "gists_url": "https://api.github.com/users/tomerk/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/tomerk/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/tomerk/subscriptions",
      "organizations_url": "https://api.github.com/users/tomerk/orgs",
      "repos_url": "https://api.github.com/users/tomerk/repos",
      "events_url": "https://api.github.com/users/tomerk/events{/privacy}",
      "received_events_url": "https://api.github.com/users/tomerk/received_events",
      "type": "User",
      "site_admin": false
    }
  ],
  "milestone": null,
  "comments": 14,
  "created_at": "2020-06-20T21:55:16Z",
  "updated_at": "2020-08-04T18:57:43Z",
  "closed_at": "2020-08-04T18:57:41Z",
  "author_association": "NONE",
  "active_lock_reason": null,
  "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab enviroment\r\n\r\n- TensorFlow installed from (source or binary): Google colab default \r\n- Python version: Python 3, Google colab default \r\n\r\n- CUDA/cuDNN version:   Google colab default  \r\n- GPU model and memory:  Tested on both Google colab p-100 GPU and CPU\r\n\r\nYou can collect some of this information using our environment capture\r\n[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with:\r\n2. TF 2.0: `python -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"`\r\n\r\n>2020-06-20 21:44:17.003371: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\r\nv2.2.0-0-g2b96f3662b 2.2.0\r\n\r\n**Describe the current behavior**\r\n\r\nI created a new model using two layers from and old model. However, now all of the layers/weights from the old model are Not showing up in the new model. \r\n\r\n`model.summary()` and \r\n\r\n```\r\ntf.keras.utils.plot_model(\r\n    model, to_file='model.png', show_shapes=False, show_layer_names=True,\r\n    rankdir='TB', expand_nested=False, dpi=96\r\n)\r\n```\r\n\r\nstill has those weights, so I think they're a part of the graph.  But when I print them out, those weights/layers are missing altogether \r\n\r\n\r\n**Describe the expected behavior**\r\n\r\nAll weights from component layers to should be in the model. \r\n\r\n**Standalone code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate\r\nthe problem. If possible, please share a link to Colab/Jupyter/any notebook.\r\n\r\nHere is a Colabnotebook with a minimal example that reproduced the issue. \r\n\r\nhttps://colab.research.google.com/drive/1n3_XNhdgH6Qo7GT-M570lIKWAoU3TML5?usp=sharing\r\n\r\nAnd here is the code\r\n\r\n```\r\n!pip install transformers --q\r\n%tensorflow_version 2.x\r\n\r\nfrom transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom copy import deepcopy\r\n\r\nlogger = tf.get_logger()\r\nlogger.info(tf.__version__)\r\n\r\n\r\ndef get_mini_models():\r\n    tempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)\r\n\r\n    layer9 = deepcopy(tempModel.layers[0].encoder.layer[8])\r\n    layer10 = deepcopy(tempModel.layers[0].encoder.layer[9])\r\n\r\n    inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',\r\n                                    batch_size=None) \r\n\r\n    hidden1 = layer9((inputHiddenVals, None, None))\r\n    hidden2 = layer10((hidden1[0], None, None))\r\n    modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=hidden2)\r\n\r\n    del tempModel\r\n\r\n    return modelNew\r\n\r\n@tf.function\r\ndef loss_fn(_, probs):\r\n    bs = tf.shape(probs)[0]\r\n    labels = tf.eye(bs, bs)\r\n    return tf.losses.categorical_crossentropy(labels,\r\n                                              probs,\r\n                                              from_logits=True)\r\n\r\nmodel = get_mini_models()\r\nmodel.compile(loss=loss_fn,\r\n                optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, \r\n                                                epsilon=1e-06))\r\n\r\n# Get model and layers directly to compare\r\ntempModel = TFRobertaModel.from_pretrained('bert-base-uncased', from_pt=True)\r\nlayer9 = deepcopy(tempModel.layers[0].encoder.layer[8])\r\nlayer10 = deepcopy(tempModel.layers[0].encoder.layer[9])\r\n\r\n# Only one layer, and that layer also has missing weights. \r\nfor i, var in enumerate(model.weights):\r\n    print(model.weights[i].name)\r\n\r\n# Full weights for one layer \r\nfor i, var in enumerate(layer9.weights):\r\n    print(layer9.weights[i].name)\r\n\r\n# Test what correct output should be \r\n\r\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\r\ninputt = tokenizer.encode('This is a sentence', return_tensors='tf')\r\noutt = tempModel(inputt)[0]\r\n\r\n# Test model output. Not the same. \r\n\r\nmodel(outt)\r\n\r\n# Model summary somehow lists the weights \r\nmodel.summary()\r\n\r\n# Model diagram shows the correct connections between all the layers. \r\n\r\ntf.keras.utils.plot_model(\r\n    model, to_file='model.png', show_shapes=False, show_layer_names=True,\r\n    rankdir='TB', expand_nested=False, dpi=96\r\n)\r\n\r\n```\r\n\r\nEdit: I also tried making the layers from scratch, and setting the weights directly, and got the same result. Here's a colab notebook that does this. https://colab.research.google.com/drive/1EC_fObSp9lUsj_PFaYgFtRI93ErPYmU9?usp=sharing\r\n\r\nAnd here's the code \r\n\r\n```\r\n!pip install transformers --q\r\n%tensorflow_version 2.x\r\n\r\nfrom transformers import TFBertModel, AutoModel, TFRobertaModel, AutoTokenizer\r\n\r\nimport tensorflow as tf\r\nimport tensorflow_addons as tfa\r\n\r\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n\r\nfrom tensorflow import keras\r\nfrom tensorflow.keras import layers\r\nfrom tensorflow.keras.layers import (Dense,\r\n                                     Dropout)\r\nimport numpy as np\r\nimport os\r\n\r\nlogger = tf.get_logger()\r\nlogger.info(tf.__version__)\r\n\r\nclass TFBertSelfAttention2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        if config.hidden_size % config.num_attention_heads != 0:\r\n            raise ValueError(\r\n                \"The hidden size (%d) is not a multiple of the number of attention \"\r\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\r\n            )\r\n\r\n        self.num_attention_heads = config.num_attention_heads\r\n        assert config.hidden_size % config.num_attention_heads == 0\r\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n\r\n        self.query = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query_2\"\r\n        )\r\n        self.key = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key_2\"\r\n        )\r\n        self.value = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value_2\"\r\n        )\r\n\r\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\r\n\r\n    def transpose_for_scores(self, x, batch_size):\r\n        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        batch_size = shape_list(hidden_states)[0]\r\n        mixed_query_layer = self.query(hidden_states)\r\n        mixed_key_layer = self.key(hidden_states)\r\n        mixed_value_layer = self.value(hidden_states)\r\n\r\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\r\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\r\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\r\n\r\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\r\n        attention_scores = tf.matmul(\r\n            query_layer, key_layer, transpose_b=True\r\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\r\n        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\r\n        attention_scores = attention_scores / tf.math.sqrt(dk)\r\n\r\n        if attention_mask is not None:\r\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\r\n            attention_scores = attention_scores + attention_mask\r\n\r\n        # Normalize the attention scores to probabilities.\r\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\r\n\r\n        # This is actually dropping out entire tokens to attend to, which might\r\n        # seem a bit unusual, but is taken from the original Transformer paper.\r\n        attention_probs = self.dropout(attention_probs, training=training)\r\n\r\n        # Mask heads if we want to\r\n        if head_mask is not None:\r\n            attention_probs = attention_probs * head_mask\r\n\r\n        context_layer = tf.matmul(attention_probs, value_layer)\r\n\r\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\r\n        context_layer = tf.reshape(\r\n            context_layer, (batch_size, -1, self.all_head_size)\r\n        )  # (batch_size, seq_len_q, all_head_size)\r\n\r\n        outputs = (\r\n            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\r\n        )\r\n\r\n        return outputs\r\n\r\n\r\nclass TFBertSelfOutput2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense2\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm2\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertAttention2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.self_attention = TFBertSelfAttention2(config, name=\"self2\")\r\n        self.dense_output = TFBertSelfOutput2(config, name=\"output2\")\r\n\r\n    def prune_heads(self, heads):\r\n        raise NotImplementedError\r\n\r\n    def call(self, inputs, training=False):\r\n        input_tensor, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        self_outputs = self.self_attention(\r\n            [input_tensor, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\r\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFBertIntermediate2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense2\"\r\n        )\r\n        if isinstance(config.hidden_act, str):\r\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\r\n        else:\r\n            self.intermediate_act_fn = config.hidden_act\r\n\r\n    def call(self, hidden_states):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.intermediate_act_fn(hidden_states)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertOutput2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense2\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm2\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertLayer2(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.attention = TFBertAttention2(config, name=\"attention2\")\r\n        self.intermediate = TFBertIntermediate2(config, name=\"intermediate2\")\r\n        self.bert_output = TFBertOutput2(config, name=\"output2\")\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        attention_outputs = self.attention(\r\n            [hidden_states, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = attention_outputs[0]\r\n        intermediate_output = self.intermediate(attention_output)\r\n        layer_output = self.bert_output([intermediate_output, attention_output], training=training)\r\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFBertSelfAttention(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        if config.hidden_size % config.num_attention_heads != 0:\r\n            raise ValueError(\r\n                \"The hidden size (%d) is not a multiple of the number of attention \"\r\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\r\n            )\r\n\r\n        self.num_attention_heads = config.num_attention_heads\r\n        assert config.hidden_size % config.num_attention_heads == 0\r\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n\r\n        self.query = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query_\"\r\n        )\r\n        self.key = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key_\"\r\n        )\r\n        self.value = tf.keras.layers.Dense(\r\n            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value_\"\r\n        )\r\n\r\n        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\r\n\r\n    def transpose_for_scores(self, x, batch_size):\r\n        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\r\n        return tf.transpose(x, perm=[0, 2, 1, 3])\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        batch_size = shape_list(hidden_states)[0]\r\n        mixed_query_layer = self.query(hidden_states)\r\n        mixed_key_layer = self.key(hidden_states)\r\n        mixed_value_layer = self.value(hidden_states)\r\n\r\n        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\r\n        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\r\n        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\r\n\r\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\r\n        attention_scores = tf.matmul(\r\n            query_layer, key_layer, transpose_b=True\r\n        )  # (batch size, num_heads, seq_len_q, seq_len_k)\r\n        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\r\n        attention_scores = attention_scores / tf.math.sqrt(dk)\r\n\r\n        if attention_mask is not None:\r\n            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\r\n            attention_scores = attention_scores + attention_mask\r\n\r\n        # Normalize the attention scores to probabilities.\r\n        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\r\n\r\n        # This is actually dropping out entire tokens to attend to, which might\r\n        # seem a bit unusual, but is taken from the original Transformer paper.\r\n        attention_probs = self.dropout(attention_probs, training=training)\r\n\r\n        # Mask heads if we want to\r\n        if head_mask is not None:\r\n            attention_probs = attention_probs * head_mask\r\n\r\n        context_layer = tf.matmul(attention_probs, value_layer)\r\n\r\n        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\r\n        context_layer = tf.reshape(\r\n            context_layer, (batch_size, -1, self.all_head_size)\r\n        )  # (batch_size, seq_len_q, all_head_size)\r\n\r\n        outputs = (\r\n            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\r\n        )\r\n\r\n        return outputs\r\n\r\n\r\nclass TFBertSelfOutput(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertAttention(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.self_attention = TFBertSelfAttention(config, name=\"self\")\r\n        self.dense_output = TFBertSelfOutput(config, name=\"output\")\r\n\r\n    def prune_heads(self, heads):\r\n        raise NotImplementedError\r\n\r\n    def call(self, inputs, training=False):\r\n        input_tensor, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        self_outputs = self.self_attention(\r\n            [input_tensor, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\r\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\n\r\nclass TFBertIntermediate(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        if isinstance(config.hidden_act, str):\r\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\r\n        else:\r\n            self.intermediate_act_fn = config.hidden_act\r\n\r\n    def call(self, hidden_states):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.intermediate_act_fn(hidden_states)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertOutput(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.dense = tf.keras.layers.Dense(\r\n            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\r\n        )\r\n        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\r\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, input_tensor = inputs\r\n\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states, training=training)\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass TFBertLayer(tf.keras.layers.Layer):\r\n    def __init__(self, config, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.attention = TFBertAttention(config, name=\"attention\")\r\n        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\r\n        self.bert_output = TFBertOutput(config, name=\"output\")\r\n\r\n    def call(self, inputs, training=False):\r\n        hidden_states, attention_mask, head_mask, output_attentions = inputs\r\n\r\n        attention_outputs = self.attention(\r\n            [hidden_states, attention_mask, head_mask, output_attentions], training=training\r\n        )\r\n        attention_output = attention_outputs[0]\r\n        intermediate_output = self.intermediate(attention_output)\r\n        layer_output = self.bert_output([intermediate_output, attention_output], training=training)\r\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\nconfigBase = {\r\n  \"attention_probs_dropout_prob\": 0.1,\r\n  \"bos_token_id\": 0,\r\n  \"eos_token_id\": 2,\r\n  \"hidden_act\": \"gelu\",\r\n  \"hidden_dropout_prob\": 0.1,\r\n  \"hidden_size\": 768,\r\n  \"initializer_range\": 0.02,\r\n  \"intermediate_size\": 3072,\r\n  \"layer_norm_eps\": 1e-05,\r\n  \"max_position_embeddings\": 514,\r\n  \"model_type\": \"roberta\",\r\n  \"num_attention_heads\": 12,\r\n  \"num_hidden_layers\": 12,\r\n  \"pad_token_id\": 1,\r\n  \"type_vocab_size\": 1,\r\n  \"vocab_size\": 50265\r\n}\r\n\r\nclass AttrDict(dict):\r\n    def __init__(self, *args, **kwargs):\r\n        super(AttrDict, self).__init__(*args, **kwargs)\r\n        self.__dict__ = self\r\n\r\nconfig = AttrDict(configBase)\r\n\r\ndef get_initializer(initializer_range=0.02):\r\n    \"\"\"Creates a `tf.initializers.truncated_normal` with the given range.\r\n    Args:\r\n        initializer_range: float, initializer range for stddev.\r\n    Returns:\r\n        TruncatedNormal initializer with stddev = `initializer_range`.\r\n    \"\"\"\r\n    return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)\r\n\r\n\r\ndef gelu(x):\r\n    \"\"\" Gaussian Error Linear Unit.\r\n    Original Implementation of the gelu activation function in Google Bert repo when initially created.\r\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\r\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\r\n        Also see https://arxiv.org/abs/1606.08415\r\n    \"\"\"\r\n    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\r\n    return x * cdf\r\n\r\nACT2FN = {\r\n    \"gelu\": tf.keras.layers.Activation(gelu),\r\n}\r\n\r\ndef shape_list(x):\r\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\r\n    static = x.shape.as_list()\r\n    dynamic = tf.shape(x)\r\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\r\n\r\ndef cast_bool_to_primitive(bool_variable, default_tensor_to_true=False):\r\n    \"\"\"Function arguments can be inserted as boolean tensor\r\n        and bool variables to cope with keras serialization\r\n        we need to cast `output_attentions` to correct bool\r\n        if it is a tensor\r\n    Args:\r\n        default_tensor_to_true: bool, if tensor should default to True\r\n        in case tensor has no numpy attribute\r\n    \"\"\"\r\n    # if bool variable is tensor and has numpy value\r\n    if tf.is_tensor(bool_variable):\r\n        if hasattr(bool_variable, \"numpy\"):\r\n            return bool(bool_variable.numpy())\r\n        elif default_tensor_to_true:\r\n            return True\r\n\r\n    # else variable is bool\r\n    return bool_variable\r\n\r\ndef get_2_transformerLayerP(numb):\r\n    tokenizer = AutoTokenizer.from_pretrained('allenai/biomed_roberta_base')\r\n    inputt = tokenizer.encode('This is a sentence', return_tensors='tf')\r\n    tempModel = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\r\n    outt = tempModel(inputt)[0]\r\n\r\n    t_layer11 = TFBertLayer(config, name=\"layer_._{}\".format(11+numb))\r\n    t_layer12 = TFBertLayer2(config, name=\"layer_._{}\".format(12+numb))\r\n\r\n    t_layer11((outt, None, None, None))\r\n    t_layer12((outt, None, None, None))\r\n\r\n    t_layer11.set_weights( tempModel.layers[0].encoder.layer[10].get_weights() )\r\n    t_layer12.set_weights( tempModel.layers[0].encoder.layer[11].get_weights() )\r\n\r\n    t_layer12.intermediate.intermediate_act_fn = tf.keras.activations.tanh\r\n\r\n    del tokenizer\r\n    del tempModel\r\n\r\n    return t_layer11, t_layer12\r\n\r\ndef get_mini_models():\r\n    P_trans11, P_trans12 = get_2_transformerLayerP(6)\r\n\r\n    inputHiddenVals = tf.keras.Input(shape=[None, None], dtype=tf.float32, name='input_Q',\r\n                                    batch_size=None) \r\n\r\n    P_outputs = P_trans11((inputHiddenVals, None, None, None))[0]\r\n    P_outputsFinal = P_trans12((P_outputs, None, None, None))[0]\r\n    modelNew = tf.keras.Model(inputs=inputHiddenVals, outputs=P_outputsFinal)\r\n\r\n    return modelNew\r\n\r\n@tf.function\r\ndef loss_fn(_, probs):\r\n\r\n    bs = tf.shape(probs)[0]\r\n    labels = tf.eye(bs, bs)\r\n    return tf.losses.categorical_crossentropy(labels,\r\n                                              probs,\r\n                                              from_logits=True)\r\n\r\nmodel = get_mini_models()\r\nmodel.compile(loss=loss_fn,\r\n                optimizer=tfa.optimizers.AdamW(weight_decay=1e-4, learning_rate=1e-5, \r\n                                                epsilon=1e-06))\r\n\r\nfor i, var in enumerate(model.trainable_weights):\r\n    print(model.trainable_weights[i].name)\r\n\r\n```",
  "closed_by": {
    "login": "tomerk",
    "id": 676357,
    "node_id": "MDQ6VXNlcjY3NjM1Nw==",
    "avatar_url": "https://avatars.githubusercontent.com/u/676357?v=4",
    "gravatar_id": "",
    "url": "https://api.github.com/users/tomerk",
    "html_url": "https://github.com/tomerk",
    "followers_url": "https://api.github.com/users/tomerk/followers",
    "following_url": "https://api.github.com/users/tomerk/following{/other_user}",
    "gists_url": "https://api.github.com/users/tomerk/gists{/gist_id}",
    "starred_url": "https://api.github.com/users/tomerk/starred{/owner}{/repo}",
    "subscriptions_url": "https://api.github.com/users/tomerk/subscriptions",
    "organizations_url": "https://api.github.com/users/tomerk/orgs",
    "repos_url": "https://api.github.com/users/tomerk/repos",
    "events_url": "https://api.github.com/users/tomerk/events{/privacy}",
    "received_events_url": "https://api.github.com/users/tomerk/received_events",
    "type": "User",
    "site_admin": false
  },
  "reactions": {
    "url": "https://api.github.com/repos/tensorflow/tensorflow/issues/40638/reactions",
    "total_count": 4,
    "+1": 3,
    "-1": 0,
    "laugh": 0,
    "hooray": 0,
    "confused": 1,
    "heart": 0,
    "rocket": 0,
    "eyes": 0
  },
  "timeline_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/40638/timeline",
  "performed_via_github_app": null
}
